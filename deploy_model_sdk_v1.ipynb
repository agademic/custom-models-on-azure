{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Deploy a Custom Hugging Face Model to Azure ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a step by step guide on how to deploy a custom Hugging Face model to Azure ML. In order to be able to deploy the model from your local environment to Azure ML we will need to install the libraries from ```requirements.txt```.\n",
    "\n",
    "Then, we can run the cells in the notebook to prepare the model and deploy it to Azure ML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Download the model from Hugging Face Hub and save it locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we are going to deploy an embedding model. To keep the example as simple as possible, we will be using sentence-transformers for inference. Here, we download the model from Hugging Face Hub and save it locally to upload and register the model in Azure ML.\n",
    "\n",
    "If we already have the model locally, we can skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"BAAI/bge-m3\")\n",
    "model.save(\"path/to/model/bge-m3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Connect to Azure ML Workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest way to connect to our Azure ML Workspace without hard coding your credentials is to pass the path to the ```config.json``` file as a parameter to the ```Workspace.from_config()``` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "# Connect to your Azure ML workspace\n",
    "ws = Workspace.from_config(\"config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Register the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Registering the model means that the model will uploaded to the Azure ML Model Registry. Depending on the size of the model, this can take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Model\n",
    "\n",
    "# Register the model\n",
    "model = Model.register(workspace=ws,\n",
    "                       model_name='bge-m3',  # Give a unique name\n",
    "                       model_path=\"path/to/model/bge-m3\",  # Path to the model directory\n",
    "                       description=\"Embedding model from Hugging Face Hub\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the environment by giving it a distinct name and pass all packages you will need to run inferences with the model. In this case, we will be basically using sentence-transformers only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the environment\n",
    "from azureml.core.environment import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "env = Environment('huggingface-embeddings')\n",
    "deps = CondaDependencies.create(conda_packages=[],\n",
    "                                pip_packages=['azureml-defaults', 'sentence-transformers==2.7.0'])\n",
    "env.python.conda_dependencies = deps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Create an inference script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get inferences from the model, we need to create an inference script. For Azure ML, the script needs to have an ```init()``` and a ```run()``` function. The ```init()``` function will run only once at the start of the contrainer and is for loading the model into memory. The ```run()``` function will be called every time a request is made to the endpoint and needs to unpack the request data and pass it to the model for inference.\n",
    "\n",
    "Refer to the ```score.py``` file in this repository for an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import InferenceConfig\n",
    "\n",
    "inference_config = InferenceConfig(entry_script=\"score.py\", environment=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Create a deployment configuration and deploy the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before deploying the model, we need to define the deployment configuration. Set ```cpu_cores``` and ```memory_in_gb```, so the model can be loaded into memory and the container will not run out of memory while performing inference.\n",
    "\n",
    "The deploy method will the start the deployment process. Here, a container image will be created and a VM instance will be created to run the container. The whole process can take a while.\n",
    "\n",
    "If we already registered a model but want to redeploy it, we can grab the model to pass it to the deploy method via ```model = Model(ws, 'bge-m3')```.\n",
    "\n",
    "Double-Check that you are using the Azure ML SDK v1 or v2 for both the model registration and the deployment. If we registered the model via Studio UI or SDK v2, we can not deploy the model via SDK v1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the model\n",
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.webservice import AciWebservice\n",
    "\n",
    "inference_config = InferenceConfig(entry_script='score.py', environment=env)\n",
    "deployment_config = AciWebservice.deploy_configuration(cpu_cores=2, memory_gb=4)\n",
    "\n",
    "service = Model.deploy(workspace=ws,\n",
    "                       name='huggingface-embeddings',\n",
    "                       models=[model],\n",
    "                       inference_config=inference_config,\n",
    "                       deployment_config=deployment_config)\n",
    "service.wait_for_deployment(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Call the endpoint and get inferences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To call the endpoint, we can either send a request via http or use ```service.run()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import embed_text\n",
    "print(service.scoring_uri)\n",
    "\n",
    "test_sample = \"The quick brown fox jumps over the lazy dog.\"\n",
    "embeddings = embed_text(test_sample, service.scoring_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import Webservice\n",
    "\n",
    "service = Webservice(name='huggingface-embeddings', workspace=ws)\n",
    "\n",
    "test_sample = \"The quick brown fox jumps over the lazy dog.\"\n",
    "embeddings = service.run(test_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Now do the fun part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we got the embeddings, we can use them for all kinds of downstream tasks. For instance, we can embed multiple texts and calculate the cosine similarity between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import cosine_similarity\n",
    "\n",
    "print(cosine_similarity(embed_text(\"Bock\"), embed_text(\"B\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    \"The first human landing on the Moon was achieved in 1969.\",\n",
    "    \"Neil Armstrong was the first person to walk on the lunar surface.\",\n",
    "    \"Apollo 11 was the spaceflight that landed the first two people on the Moon.\",\n",
    "]\n",
    "query = \"Who was the first to walk on the Moon?\"\n",
    "\n",
    "for i in docs:\n",
    "    print(cosine_similarity(embed_text(query), embed_text(i)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "themis-base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
