{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Deploy a Custom Hugging Face Model to Azure ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a step by step guide on how to deploy a custom Hugging Face model to Azure ML. In order to be able to deploy the model from your local environment to Azure ML we will need to install the libraries from ```requirements.txt```.\n",
    "\n",
    "Then, we can run the cells in the notebook to prepare the model and deploy it to Azure ML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Download the model from Hugging Face Hub and save it locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we are going to deploy an embedding model. To keep the example as simple as possible, we will be using sentence-transformers for inference. Here, we download the model from Hugging Face Hub and save it locally to upload and register the model in Azure ML.\n",
    "\n",
    "If we already have the model locally, we can skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"BAAI/bge-m3\")\n",
    "model.save(\"path/to/model/bge-m3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.ai.ml.entities import (\n",
    "    ManagedOnlineEndpoint,\n",
    "    ManagedOnlineDeployment,\n",
    "    Environment,\n",
    "    CodeConfiguration,\n",
    ")\n",
    "from azure.identity import DefaultAzureCredential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 Authenticate via Azure CLI\n",
    "In order to ensure a smooth connection to the workspace down below, authenticate first by installing the Azure CLI. Please find the installation steps here: [Install Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) and authenticaing via `az login`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Connect to Azure ML Workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SDK v2 now uses the `MLClient` class to connect to a workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter details of your Azure Machine Learning workspace\n",
    "subscription_id = 'xxx'\n",
    "resource_group = 'xxx'\n",
    "workspace = 'xxx'\n",
    "\n",
    "# get a handle to the workspace\n",
    "ml_client = MLClient(\n",
    "    DefaultAzureCredential(), subscription_id, resource_group, workspace\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Register the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Registering the model means that the model will uploaded to the Azure ML Model Registry. Depending on the size of the model, this can take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Model\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "file_model = Model(\n",
    "    path=\"path/to/model/bge-m3\",\n",
    "    type=AssetTypes.CUSTOM_MODEL,\n",
    "    name=\"bge-m3\",\n",
    "    description=\"Hugging Face BGE-M3 model uploaded to Azure ML Model Registry.\",\n",
    ")\n",
    "ml_client.models.create_or_update(file_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Get the model from the registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Model object using the name and version of the already registered model\n",
    "model = ml_client.models.get(name=\"bge-m3\", version=\"1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the environment by giving it a distinct name and pass all packages you will need to run inferences with the model. Define all packages within the conda.yaml file.\n",
    "\n",
    "```yaml\n",
    "name: hugging-face-embeddings-env\n",
    "# channels:\n",
    "#   - conda-forge\n",
    "dependencies:\n",
    "  - python=3.11\n",
    "  - pip=22.1.2\n",
    "  - pip:\n",
    "    - azureml-inference-server-http==1.3.0\n",
    "    - sentence-transformers\n",
    "    - scipy==1.10.1\n",
    "```\n",
    "\n",
    "In this case, we will be basically using sentence-transformers only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment(\n",
    "    conda_file=\"azureml-models/b3_bilang_extraction_model/1/conda.yaml\",\n",
    "    image=\"mcr.microsoft.com/azureml/inference-base-2204:20240530.v1\",\n",
    ")\n",
    "\n",
    "ml_client.environments.create_or_update(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Create an inference script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get inferences from the model, we need to create an inference script. For Azure ML, the script needs to have an ```init()``` and a ```run()``` function. The ```init()``` function will run only once at the start of the contrainer and is for loading the model into memory. The ```run()``` function will be called every time a request is made to the endpoint and needs to unpack the request data and pass it to the model for inference.\n",
    "\n",
    "Refer to the ```score.py``` file in this repository for an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Create a deployment configuration and deploy the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, deploying a model to Azure ML needs an endpoint and a deployment within the endpoint. The endpoint is a webservice which can be called via REST API. It has an URI and pre-defined authentication methods, such as key-based authentication. A deployment is kind of the backbone of the webservice. The deployment is the actual container that will run the model. It has a pre-defined environment and hardware to run the container.\n",
    "\n",
    "Before deploying the model, we need to define the endpoint and the deployment configuration. Here we choose `Standard_E4s_v3` since it has enough CPU power and memory, so the model can be loaded into memory and the container will not run out of memory while performing inference.\n",
    "\n",
    "The deploy method will the start the deployment process. Here, a container image will be created and a VM instance will be created to run the container. The whole process can take a while.\n",
    "\n",
    "If we already registered a model but want to redeploy it, we can grab the model to pass it to the deploy method via `model = ml_client.models.get(name=\"bge-m3\", version=\"1\")`.\n",
    "\n",
    "Important: Double-Check that you are using the Azure ML SDK v1 or v2 for both the model registration and the deployment. If we registered the model via Studio UI or SDK v2, we can not deploy the model via SDK v1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an endpoint name\n",
    "endpoint_name = \"hugging-face-embeddings-endpoint\"\n",
    "\n",
    "# create an online endpoint\n",
    "endpoint = ManagedOnlineEndpoint(\n",
    "    name = endpoint_name, \n",
    "    description=\"this is the hugging face embeddings endpoint\",\n",
    "    auth_mode=\"key\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment = ManagedOnlineDeployment(\n",
    "    name=\"hugging-face-embeddings-deployment-1\", \n",
    "    endpoint_name=endpoint_name,\n",
    "    model=model,\n",
    "    environment=env,\n",
    "    code_configuration=CodeConfiguration(\n",
    "        code=\"path/to/code/folder\", scoring_script=\"score.py\"\n",
    "    ),\n",
    "    instance_type=\"Standard_E4s_v3\",\n",
    "    instance_count=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.online_endpoints.begin_create_or_update(endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.online_deployments.begin_create_or_update(deployment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.online_deployments.get_logs(\n",
    "    name=\"hugging-face-embeddings-deployment-1\", endpoint_name=\"hugging-face-embeddings-endpoint\", lines=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Call the endpoint and get inferences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To call the endpoint, we can either send a request via https or use `ml_client.online_endpoints.invoke()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# test the embeddings deployment with some sample data\n",
    "response = json.loads(ml_client.online_endpoints.invoke(\n",
    "    endpoint_name=endpoint_name,\n",
    "    deployment_name=\"hugging-face-embeddings-deployment-1\",\n",
    "    request_file=\"sample-request.json\",\n",
    "))\n",
    "\n",
    "embeddings = response[\"embeddings\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Now do the fun part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we got the embeddings, we can use them for all kinds of downstream tasks. For instance, we can embed multiple texts and calculate the cosine similarity between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import cosine_similarity\n",
    "\n",
    "print(cosine_similarity(embeddings, embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    \"The first human landing on the Moon was achieved in 1969.\",\n",
    "    \"Neil Armstrong was the first person to walk on the lunar surface.\",\n",
    "    \"Apollo 11 was the spaceflight that landed the first two people on the Moon.\",\n",
    "]\n",
    "query = \"Who was the first to walk on the Moon?\"\n",
    "\n",
    "for i in docs:\n",
    "    print(cosine_similarity(embed_text(query), embed_text(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "themis-base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
